{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b032a89",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "## THIS IS A TEST CODE\n",
    "import great_expectations as ge\n",
    "from great_expectations.checkpoint.checkpoint import Checkpoint\n",
    "from great_expectations.core.batch import RuntimeBatchRequest\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "from pyspark.sql.functions import col, upper, lit, current_timestamp, row_number\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.window import Window\n",
    "from uuid import uuid4\n",
    "import json\n",
    "import logging\n",
    "from typing import Dict, List, Optional, Tuple, Any\n",
    "\n",
    "\n",
    "class DataQualityValidator:\n",
    "    \"\"\"\n",
    "    A class to handle data quality validation using Great Expectations\n",
    "    and write clean data to Silver layer.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict[str, Any], spark_session=None):\n",
    "        \"\"\"\n",
    "        Initialize the Data Quality Validator\n",
    "        \n",
    "        Args:\n",
    "            config: Configuration dictionary containing paths and settings\n",
    "            spark_session: Spark session instance (if None, will try to get from globals)\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.logger = self._setup_logging()\n",
    "        self.context = ge.get_context()\n",
    "        self.data_assets_cache = {}\n",
    "        \n",
    "        # Handle Spark session\n",
    "        if spark_session is not None:\n",
    "            self.spark = spark_session\n",
    "        else:\n",
    "            # Try to get spark from globals (common in notebook environments)\n",
    "            try:\n",
    "                global spark\n",
    "                self.spark = spark\n",
    "            except NameError:\n",
    "                try:\n",
    "                    from pyspark.sql import SparkSession\n",
    "                    self.spark = SparkSession.getActiveSession()\n",
    "                    if self.spark is None:\n",
    "                        raise RuntimeError(\"No active Spark session found\")\n",
    "                except Exception as e:\n",
    "                    raise RuntimeError(f\"Could not initialize Spark session: {str(e)}\")\n",
    "    \n",
    "        # Set required Spark configurations\n",
    "        self.spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"false\")\n",
    "        self.spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInRead\", \"LEGACY\")\n",
    "        self.spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"LEGACY\")    \n",
    "\n",
    "        # Suppress GE logging\n",
    "        logging.getLogger(\"great_expectations\").setLevel(logging.ERROR)\n",
    "        \n",
    "    def _setup_logging(self) -> logging.Logger:\n",
    "        \"\"\"Setup logging configuration\"\"\"\n",
    "        logger = logging.getLogger(self.__class__.__name__)\n",
    "        logger.setLevel(self.config.get('log_level', logging.INFO))\n",
    "        \n",
    "        if not logger.handlers:\n",
    "            handler = logging.StreamHandler()\n",
    "            formatter = logging.Formatter(\n",
    "                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "            )\n",
    "            handler.setFormatter(formatter)\n",
    "            logger.addHandler(handler)\n",
    "            \n",
    "        return logger\n",
    "    \n",
    "    def load_validation_expectations(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load validation expectations from the configuration table\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame containing validation expectations for selected tables\n",
    "        \"\"\"\n",
    "        expectations_df = (\n",
    "            self.spark.read.format(\"delta\")\n",
    "            .load(self.config['expectations_table_path'])\n",
    "            .filter(upper(col(\"Selected\").cast(\"string\")) == \"TRUE\")\n",
    "            .toPandas()\n",
    "        )\n",
    "        \n",
    "        self.logger.info(f\"Loaded {len(expectations_df)} validation expectations\")\n",
    "        return expectations_df\n",
    "    \n",
    "    def get_selected_tables(self, expectations_df: pd.DataFrame) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get list of selected tables for processing\n",
    "        \n",
    "        Args:\n",
    "            expectations_df: DataFrame containing validation expectations\n",
    "            \n",
    "        Returns:\n",
    "            List of table names to process\n",
    "        \"\"\"\n",
    "        selected_tables = expectations_df['Table_Name'].unique().tolist()\n",
    "        self.logger.info(f\"Found {len(selected_tables)} tables to process\")\n",
    "        return selected_tables\n",
    "    \n",
    "    def create_data_source(self, table_name: str) -> str:\n",
    "        \"\"\"\n",
    "        Create a Great Expectations data source for the table\n",
    "        \n",
    "        Args:\n",
    "            table_name: Name of the table\n",
    "            \n",
    "        Returns:\n",
    "            Data source name\n",
    "        \"\"\"\n",
    "        data_source_name = f\"spark_src_{table_name}_{uuid4().hex[:8]}\"\n",
    "        \n",
    "        self.context.add_datasource(\n",
    "            name=data_source_name,\n",
    "            class_name=\"Datasource\",\n",
    "            execution_engine={\"class_name\": \"SparkDFExecutionEngine\"},\n",
    "            data_connectors={\n",
    "                \"default_runtime_data_connector_name\": {\n",
    "                    \"class_name\": \"RuntimeDataConnector\",\n",
    "                    \"batch_identifiers\": [\"default_identifier_name\"],\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        return data_source_name\n",
    "    \n",
    "    def load_and_prepare_data(self, table_name: str) -> Any:\n",
    "        \"\"\"\n",
    "        Load and prepare data for validation\n",
    "        \n",
    "        Args:\n",
    "            table_name: Name of the table to load\n",
    "            \n",
    "        Returns:\n",
    "            Spark DataFrame with prepared data\n",
    "        \"\"\"\n",
    "        # Load data\n",
    "        table_path = f\"{self.config['source_base_path']}/{table_name}\"\n",
    "        clean_df = self.spark.read.format(\"delta\").load(table_path)\n",
    "        \n",
    "        # Remove null columns\n",
    "        non_null_cols = [\n",
    "            c for c in clean_df.columns \n",
    "            if clean_df.filter(f\"{c} IS NOT NULL\").limit(1).count() > 0\n",
    "        ]\n",
    "        clean_df = clean_df.select(*non_null_cols)\n",
    "        \n",
    "        # Filter by delete flag if exists\n",
    "        delete_flag_column = next(\n",
    "            (c for c in clean_df.columns if 'delete_flag' in c.lower()), \n",
    "            None\n",
    "        )\n",
    "        if delete_flag_column:\n",
    "            clean_df = clean_df.filter(col(delete_flag_column) == 0)\n",
    "            self.logger.info(f\"Filtered {delete_flag_column} == 0 for table {table_name}\")\n",
    "        \n",
    "        # Add row index for tracking\n",
    "        indexed_df = clean_df.withColumn(\n",
    "            \"__row_idx\", \n",
    "            row_number().over(Window.orderBy(lit(1))) - 1\n",
    "        )\n",
    "        \n",
    "        self.logger.info(f\"Prepared {indexed_df.count()} rows for table {table_name}\")\n",
    "        return indexed_df\n",
    "    \n",
    "    def create_expectation_metadata(self, expectation_for_table: pd.DataFrame) -> Dict[Tuple, Dict]:\n",
    "        \"\"\"\n",
    "        Create metadata mapping for expectations\n",
    "        \n",
    "        Args:\n",
    "            expectation_for_table: DataFrame containing expectations for current table\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary mapping (rule, column) to metadata\n",
    "        \"\"\"\n",
    "        expectation_metadata_map = {}\n",
    "        \n",
    "        for _, row in expectation_for_table.iterrows():\n",
    "            col_nm = row['Column_Name'].strip()\n",
    "            rule = row['validation_rule'].strip()\n",
    "            cond = row['Condition'].strip() if pd.notna(row['Condition']) else None\n",
    "            \n",
    "            meta = {\n",
    "                \"Source_System\": self.config.get('source_system', 'Netforum'),\n",
    "                \"Table_Name\": row.get('Table_Name', ''),\n",
    "                \"Column_Name\": col_nm,\n",
    "                \"AI_ColumnName\": row.get('AI_ColumnName', \"\"),\n",
    "                \"Index_Key\": row.get('Index_Key', \"\"),\n",
    "                \"Val_Description\": row.get('Val_Description', \"\"),\n",
    "                \"Condition\": cond,\n",
    "                \"Minimum_Range\": row.get('Minimum_Range'),\n",
    "                \"Maximum_Range\": row.get('Maximum_Range'),\n",
    "                \"AI_Reasoning\": row.get('AI_Reasoning', \"\"),\n",
    "                \"SampleData\": row.get('SampleData', \"\"),\n",
    "            }\n",
    "            expectation_metadata_map[(rule, col_nm)] = meta\n",
    "            \n",
    "        return expectation_metadata_map\n",
    "    \n",
    "    def add_expectations_to_validator(self, validator: Any, expectation_for_table: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Add expectations to the validator based on validation rules\n",
    "        \n",
    "        Args:\n",
    "            validator: Great Expectations validator\n",
    "            expectation_for_table: DataFrame containing expectations for current table\n",
    "        \"\"\"\n",
    "        for _, row in expectation_for_table.iterrows():\n",
    "            col_nm = row['Column_Name'].strip()\n",
    "            rule = row['validation_rule'].strip()\n",
    "            cond = row['Condition'].strip() if pd.notna(row['Condition']) else None\n",
    "            \n",
    "            if rule == 'expect_column_values_to_not_be_null':\n",
    "                validator.expect_column_values_to_not_be_null(column=col_nm)\n",
    "                \n",
    "            elif rule == 'expect_column_values_to_match_regex' and cond:\n",
    "                validator.expect_column_values_to_match_regex(column=col_nm, regex=cond)\n",
    "                \n",
    "            elif rule == 'expect_column_values_to_be_unique':\n",
    "                validator.expect_column_values_to_be_unique(column=col_nm)\n",
    "                \n",
    "            elif rule == 'expect_column_values_to_be_between':\n",
    "                validator.expect_column_values_to_be_between(\n",
    "                    column=col_nm,\n",
    "                    min_value=row['Minimum_Range'],\n",
    "                    max_value=row['Maximum_Range']\n",
    "                )\n",
    "                \n",
    "            elif rule == 'expect_column_values_to_be_in_set' and cond:\n",
    "                raw_vals = cond.strip()\n",
    "                if raw_vals.startswith(\"[\") and raw_vals.endswith(\"]\"):\n",
    "                    raw_vals = raw_vals[1:-1]\n",
    "                \n",
    "                value_set = [int(v.strip()) for v in raw_vals.split(',')]\n",
    "                validator.expect_column_values_to_be_in_set(\n",
    "                    column=col_nm,\n",
    "                    value_set=value_set\n",
    "                )\n",
    "    \n",
    "    def process_validation_results(self, validation_result: Any, indexed_df: Any, \n",
    "                                 expectation_metadata_map: Dict, table_name: str) -> Tuple[Any, List[int]]:\n",
    "        \"\"\"\n",
    "        Process validation results and handle failed records\n",
    "        \n",
    "        Args:\n",
    "            validation_result: Great Expectations validation result\n",
    "            indexed_df: Indexed DataFrame\n",
    "            expectation_metadata_map: Metadata mapping for expectations\n",
    "            table_name: Name of the current table\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (clean_df, all_unexpected_indices)\n",
    "        \"\"\"\n",
    "        all_unexpected_indices = []\n",
    "        failed_columns = set()\n",
    "        \n",
    "        # Extract failed indices and columns\n",
    "        for result in validation_result.to_json_dict()[\"results\"]:\n",
    "            col_name = result.get(\"expectation_config\", {}).get(\"kwargs\", {}).get(\"column\")\n",
    "            unexpected_list = result[\"result\"].get(\"unexpected_index_list\", [])\n",
    "            \n",
    "            if unexpected_list and col_name:\n",
    "                failed_columns.add(col_name)\n",
    "                for entry in unexpected_list:\n",
    "                    if \"__row_idx\" in entry:\n",
    "                        all_unexpected_indices.append(entry[\"__row_idx\"])\n",
    "        \n",
    "        if failed_columns:\n",
    "            self.logger.warning(f\"Columns that failed validation in {table_name}: {sorted(failed_columns)}\")\n",
    "        \n",
    "        # Process failed records if any\n",
    "        if all_unexpected_indices:\n",
    "            self._write_audit_records(\n",
    "                validation_result, indexed_df, expectation_metadata_map, \n",
    "                table_name, set(all_unexpected_indices)\n",
    "            )\n",
    "            \n",
    "            # Remove failed records\n",
    "            unexpected_indices_df = self.spark.createDataFrame(\n",
    "                [(i,) for i in set(all_unexpected_indices)], [\"__row_idx\"]\n",
    "            )\n",
    "            indexed_df = indexed_df.join(unexpected_indices_df, on=\"__row_idx\", how=\"left_anti\")\n",
    "        \n",
    "        clean_df = indexed_df.drop(\"__row_idx\")\n",
    "        return clean_df, all_unexpected_indices\n",
    "    \n",
    "    def _write_audit_records(self, validation_result: Any, indexed_df: Any, \n",
    "                           expectation_metadata_map: Dict, table_name: str, \n",
    "                           unexpected_index_set: set) -> None:\n",
    "        \"\"\"\n",
    "        Write audit records for failed validations\n",
    "        \n",
    "        Args:\n",
    "            validation_result: Great Expectations validation result\n",
    "            indexed_df: Indexed DataFrame\n",
    "            expectation_metadata_map: Metadata mapping for expectations\n",
    "            table_name: Name of the current table\n",
    "            unexpected_index_set: Set of unexpected indices\n",
    "        \"\"\"\n",
    "        unexpected_indices_df = self.spark.createDataFrame(\n",
    "            [(i,) for i in unexpected_index_set], [\"__row_idx\"]\n",
    "        )\n",
    "        bad_rows_df = indexed_df.join(unexpected_indices_df, on=\"__row_idx\", how=\"inner\")\n",
    "        \n",
    "        source_system_col = (\n",
    "            col(\"Source_System\") if \"Source_System\" in indexed_df.columns \n",
    "            else lit(self.config.get('source_system', 'Netforum'))\n",
    "        )\n",
    "        \n",
    "        for result in validation_result.to_json_dict()[\"results\"]:\n",
    "            exp_config = result.get(\"expectation_config\", {})\n",
    "            col_name = exp_config.get(\"kwargs\", {}).get(\"column\", \"unknown_column\")\n",
    "            exp_type = exp_config.get(\"expectation_type\", \"unknown_expectation\")\n",
    "            \n",
    "            key = (exp_type, col_name)\n",
    "            meta = expectation_metadata_map.get(key, {})\n",
    "            \n",
    "            # Handle potential null index key\n",
    "            index_key = meta.get(\"Index_Key\", \"\")\n",
    "            index_value_col = col(index_key).cast(\"string\") if index_key and index_key in bad_rows_df.columns else lit(\"\")\n",
    "            \n",
    "            audit_spark_df = bad_rows_df.select(\n",
    "                source_system_col.alias(\"Source_System\"),\n",
    "                lit(table_name).alias(\"Table_Name\"),\n",
    "                lit(col_name).alias(\"Column_Name\"),\n",
    "                lit(meta.get(\"AI_ColumnName\", \"\")).alias(\"AI_ColumnName\"),\n",
    "                lit(index_key).alias(\"Index_Key\"),\n",
    "                index_value_col.alias(\"Index_Value\"),\n",
    "                # col(col_name).alias(\"Failed_Value\"),  # Cast to string for consistency\n",
    "                lit(exp_type).alias(\"validation_rules\"),\n",
    "                lit(meta.get(\"Val_Description\", \"\")).alias(\"Val_Description\"),\n",
    "                lit(meta.get(\"AI_Reasoning\", \"\")).alias(\"ai_reasoning\"),\n",
    "                lit(meta.get(\"SampleData\", \"\")).alias(\"SampleData\"),\n",
    "                current_timestamp().alias(\"LoadTime_UTC\"),\n",
    "                current_timestamp().alias(\"LoadTime_PST\")\n",
    "            )\n",
    "            \n",
    "            # Write with schema evolution enabled\n",
    "            audit_spark_df.write \\\n",
    "                .mode(\"append\") \\\n",
    "                .option(\"overwriteSchema\", \"true\") \\\n",
    "                .saveAsTable(self.config['audit_table_name'])\n",
    "            \n",
    "            self.logger.info(f\"Written {audit_spark_df.count()} audit records for {table_name}\")\n",
    "    \n",
    "    def write_to_silver_layer(self, clean_df: Any, table_name: str) -> None:\n",
    "        \"\"\"\n",
    "        Write clean data to Silver layer\n",
    "        \n",
    "        Args:\n",
    "            clean_df: Clean DataFrame\n",
    "            table_name: Name of the table\n",
    "        \"\"\"\n",
    "        if clean_df.rdd.isEmpty():\n",
    "            self.logger.warning(f\"Clean DataFrame for {table_name} is empty after validation\")\n",
    "            return\n",
    "        \n",
    "        # Generate silver table name and path\n",
    "        base_table_name = table_name\n",
    "        if base_table_name.lower().startswith(\"bronze_\"):\n",
    "            base_table_name = base_table_name[7:]\n",
    "        \n",
    "        silver_table_name = f\"SilverStage_{base_table_name}\"\n",
    "        silver_table_path = f\"{self.config['silver_base_path']}/{silver_table_name}\"\n",
    "        \n",
    "        # Write to silver layer\n",
    "        clean_df.write.format(\"delta\") \\\n",
    "            .mode(\"overwrite\") \\\n",
    "            .option(\"overwriteSchema\", \"true\") \\\n",
    "            .save(silver_table_path)\n",
    "        \n",
    "        self.logger.info(f\"Written clean data to Silver Layer: {silver_table_path}\")\n",
    "    \n",
    "    def record_passed_table_info(self, table_name: str, row_count: int) -> None:\n",
    "        \"\"\"\n",
    "        Record information about tables that passed all validations\n",
    "        \n",
    "        Args:\n",
    "            table_name: Name of the table\n",
    "            row_count: Number of rows in the clean table\n",
    "        \"\"\"\n",
    "        load_time_pst = datetime.now()\n",
    "        \n",
    "        data = {\n",
    "            \"Table_Name\": [table_name],\n",
    "            \"LoadDate_PST\": [load_time_pst],\n",
    "            \"Count\": [row_count]\n",
    "        }\n",
    "        \n",
    "        pdf = pd.DataFrame(data)\n",
    "        sdf = self.spark.createDataFrame(pdf)\n",
    "        \n",
    "        sdf = sdf.select(\n",
    "            col(\"Table_Name\").cast(\"string\"),\n",
    "            col(\"LoadDate_PST\").cast(\"timestamp\"),\n",
    "            col(\"Count\").cast(\"long\")\n",
    "        )\n",
    "        \n",
    "        sdf.write.mode(\"append\").option(\"overwriteSchema\", \"true\").save(\n",
    "            self.config['passed_tables_path']\n",
    "        )\n",
    "        \n",
    "        self.logger.info(f\"Table {table_name} passed all DQ checks and logged to passed table\")\n",
    "    \n",
    "    def validate_single_table(self, table_name: str, expectations_df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Validate a single table\n",
    "        \n",
    "        Args:\n",
    "            table_name: Name of the table to validate\n",
    "            expectations_df: DataFrame containing all validation expectations\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(f\"Processing table: {table_name}\")\n",
    "            \n",
    "            # Get expectations for this table\n",
    "            expectation_for_table = expectations_df[expectations_df['Table_Name'] == table_name]\n",
    "            if expectation_for_table.empty:\n",
    "                self.logger.warning(f\"No expectations found for table {table_name}\")\n",
    "                return\n",
    "            \n",
    "            # Create suite and data source\n",
    "            suite_name = f\"suite_{table_name}_{uuid4().hex[:8]}\"\n",
    "            suite = self.context.create_expectation_suite(suite_name, overwrite_existing=True)\n",
    "            data_source_name = self.create_data_source(table_name)\n",
    "            \n",
    "            # Load and prepare data\n",
    "            indexed_df = self.load_and_prepare_data(table_name)\n",
    "            \n",
    "            # Create expectation metadata\n",
    "            expectation_metadata_map = self.create_expectation_metadata(expectation_for_table)\n",
    "            \n",
    "            # Create validator\n",
    "            runtime_batch_request = RuntimeBatchRequest(\n",
    "                datasource_name=data_source_name,\n",
    "                data_connector_name=\"default_runtime_data_connector_name\",\n",
    "                data_asset_name=f\"{table_name}_asset\",\n",
    "                runtime_parameters={\"batch_data\": indexed_df},\n",
    "                batch_identifiers={\"default_identifier_name\": f\"id_{uuid4().hex[:8]}\"}\n",
    "            )\n",
    "            \n",
    "            validator = self.context.get_validator(\n",
    "                batch_request=runtime_batch_request,\n",
    "                expectation_suite_name=suite_name\n",
    "            )\n",
    "            \n",
    "            # Add expectations\n",
    "            self.add_expectations_to_validator(validator, expectation_for_table)\n",
    "            \n",
    "            # Check if any expectations were added\n",
    "            if not validator.get_expectation_suite().expectations:\n",
    "                self.logger.warning(f\"No expectations added for {table_name}, skipping\")\n",
    "                return\n",
    "            \n",
    "            # Run validation\n",
    "            validation_result = validator.validate(\n",
    "                result_format={\n",
    "                    \"result_format\": \"COMPLETE\",\n",
    "                    \"unexpected_index_column_names\": [\"__row_idx\"],\n",
    "                    \"unexpected_index_list\": True\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            # Process results\n",
    "            clean_df, all_unexpected_indices = self.process_validation_results(\n",
    "                validation_result, indexed_df, expectation_metadata_map, table_name\n",
    "            )\n",
    "            \n",
    "            # Write to silver layer\n",
    "            self.write_to_silver_layer(clean_df, table_name)\n",
    "            \n",
    "            # Record passed table if no failures\n",
    "            if not all_unexpected_indices:\n",
    "                self.record_passed_table_info(table_name, clean_df.count())\n",
    "            else:\n",
    "                self.logger.info(f\"Table {table_name} had failed rows, not logging to passed table\")\n",
    "                \n",
    "            self.logger.info(f\"Completed processing {table_name} with {clean_df.count()} clean records\")\n",
    "            \n",
    "        except Exception as ex:\n",
    "            self.logger.error(f\"Error processing table {table_name}: {str(ex)}\")\n",
    "            raise\n",
    "    \n",
    "    def run_validation_pipeline(self) -> None:\n",
    "        \"\"\"\n",
    "        Run the complete validation pipeline for all selected tables\n",
    "        \"\"\"\n",
    "        try:\n",
    "            self.logger.info(\"Starting data quality validation pipeline\")\n",
    "            \n",
    "            # Load validation expectations\n",
    "            expectations_df = self.load_validation_expectations()\n",
    "            \n",
    "            # Get selected tables\n",
    "            selected_tables = self.get_selected_tables(expectations_df)\n",
    "            \n",
    "            # Process each table\n",
    "            for table_name in selected_tables:\n",
    "                self.validate_single_table(table_name, expectations_df)\n",
    "            \n",
    "            self.logger.info(\"Data quality validation pipeline completed successfully\")\n",
    "            \n",
    "        except Exception as ex:\n",
    "            self.logger.error(f\"Pipeline failed: {str(ex)}\")\n",
    "            raise"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
